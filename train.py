import torch
import torch.nn as nn
import torch.optim as optim
import numpy as np
import random
from collections import deque
import traffic_sim_env  # å¼•ç”¨ç’°å¢ƒ
import pygame           # ç”¨ä¾†åµæ¸¬æŒ‰éµ
import os

# --- è¶…åƒæ•¸è¨­å®š ---
BATCH_SIZE = 64
LR = 0.001
GAMMA = 0.99
# é€™è£¡è¨­å®šçš„æ˜¯ã€Œå¦‚æœæ˜¯æ–°éŠæˆ²ã€çš„æ•¸å€¼
EPSILON_START = 1.0
EPSILON_END = 0.01
EPSILON_DECAY = 0.995 
MEMORY_SIZE = 10000
TARGET_UPDATE = 10

class DQN(nn.Module):
    def __init__(self, input_size, output_size):
        super(DQN, self).__init__()
        self.net = nn.Sequential(
            nn.Linear(input_size, 128),
            nn.ReLU(),
            nn.Linear(128, 128),
            nn.ReLU(),
            nn.Linear(128, output_size)
        )
    def forward(self, x): return self.net(x)

def train():
    env = traffic_sim_env.TrafficSim()
    
    # Lv3 è¼¸å…¥ç¶­åº¦
    n_states = 50  
    n_actions = 5

    policy_net = DQN(n_states, n_actions)
    target_net = DQN(n_states, n_actions)
    
    epsilon = EPSILON_START
    loaded_save = False

    # --- [é—œéµä¿®æ”¹] æ™ºæ…§è®€æª”ç³»çµ± ---
    if os.path.exists("traffic_dqn.pth"):
        try:
            saved_state = torch.load("traffic_dqn.pth")
            policy_net.load_state_dict(saved_state)
            target_net.load_state_dict(saved_state)
            loaded_save = True
            print("âœ… æˆåŠŸè®€å–å­˜æª”ï¼")
            
            # [é‡é»] å¦‚æœæ˜¯è®€æª”ï¼Œä¸è¦å¾é ­è®Šç¬¨ (1.0)ï¼Œè€Œæ˜¯å¾ 0.3 é–‹å§‹
            # é€™æ¨£å®ƒæœƒä¿ç•™å¤§éƒ¨åˆ†å¯¦åŠ›ï¼ŒåŒæ™‚é‚„èƒ½ç¹¼çºŒå­¸ç¿’
            epsilon = 0.3 
            print(f"ğŸ‘‰ ç¹¼æ‰¿è¨“ç·´æ¨¡å¼ï¼šEpsilon èµ·å§‹å€¼è¨­ç‚º {epsilon}")
            
        except Exception as e:
            print(f"âš ï¸ å­˜æª”æ ¼å¼ä¸ç¬¦æˆ–æå£ï¼Œå°‡é‡æ–°è¨“ç·´: {e}")
            # å¦‚æœè®€æª”å¤±æ•—ï¼Œå»ºè­°åˆªé™¤èˆŠæª”ä»¥å…ä¸‹æ¬¡åˆå ±éŒ¯
            # os.remove("traffic_dqn.pth")
    else:
        print("ğŸ†• æ‰¾ä¸åˆ°å­˜æª”ï¼Œé–‹å§‹å…¨æ–°çš„è¨“ç·´...")
        target_net.load_state_dict(policy_net.state_dict())

    target_net.eval()
    optimizer = optim.Adam(policy_net.parameters(), lr=LR)
    loss_func = nn.MSELoss()
    
    memory = deque(maxlen=MEMORY_SIZE)
    
    # --- æ§åˆ¶é–‹é—œ ---
    render_mode = False  
    
    print("---------------------------------------------")
    print("ğŸš€ Lv3 è¨“ç·´é–‹å§‹ï¼")
    print("ğŸ‘‰ æŒ‰ä¸‹ [Z] éµï¼šåˆ‡æ›ã€Œçœ‹ç•«é¢ã€æˆ–ã€ŒèƒŒæ™¯æ¥µé€Ÿè¨“ç·´ã€")
    print("ğŸ‘‰ æŒ‰ä¸‹ [S] éµï¼šå­˜æª”ä¸¦é›¢é–‹ (Pause)")
    print("---------------------------------------------")

    episodes = 0
    steps_done = 0

    try:
        while True:
            state = env.reset()
            state = torch.FloatTensor(state).unsqueeze(0)
            total_reward = 0
            done = False
            
            while not done:
                steps_done += 1

                for event in pygame.event.get():
                    if event.type == pygame.QUIT:
                        print("ä½¿ç”¨è€…é—œé–‰è¦–çª—ï¼Œåœæ­¢è¨“ç·´ã€‚")
                        return
                    if event.type == pygame.KEYDOWN:
                        if event.key == pygame.K_z:
                            render_mode = not render_mode
                            mode_text = "ğŸ“º è§€çœ‹æ¨¡å¼" if render_mode else "ğŸš€ æ¥µé€Ÿæ¨¡å¼"
                            print(f"åˆ‡æ›æ¨¡å¼: {mode_text}")
                        if event.key == pygame.K_s:
                            print("\nğŸ’¾ å­˜æª”ä¸­...")
                            torch.save(policy_net.state_dict(), "traffic_dqn.pth")
                            print("âœ… å­˜æª”å®Œæˆï¼æ‚¨ç¾åœ¨å¯ä»¥å®‰å…¨é€€å‡ºäº†ã€‚")
                            # é€™è£¡æ‚¨å¯ä»¥é¸æ“‡è¦ä¸è¦ç›´æ¥é€€å‡ºï¼Œæˆ–è€…ç¹¼çºŒ
                            # return 

                # æ±ºå®šå‹•ä½œ
                if random.random() < epsilon:
                    action = random.randint(0, n_actions - 1)
                else:
                    with torch.no_grad():
                        action = policy_net(state).argmax().item()

                # åŸ·è¡Œå‹•ä½œ
                next_state_np, reward, done = env.step(action)
                next_state = torch.FloatTensor(next_state_np).unsqueeze(0)
                
                # è¨˜æ†¶
                memory.append((state, action, reward, next_state, done))
                state = next_state
                total_reward += reward

                # ç¹ªåœ–
                if render_mode:
                    info = [f"Ep: {episodes}", f"Eps: {epsilon:.2f}", f"Rwd: {total_reward:.1f}"]
                    env.draw(extra_info=info)
                    env.clock.tick(60)
                else:
                    if steps_done % 1000 == 0:
                         env.draw(extra_info=["TURBO MODE", f"Ep: {episodes}"])
                         print(f"\rEpisode: {episodes}, Reward: {total_reward:.1f}, Epsilon: {epsilon:.2f}", end="")

                # å­¸ç¿’ (æ¯5æ­¥)
                if len(memory) > BATCH_SIZE and steps_done % 5 == 0:
                    batch = random.sample(memory, BATCH_SIZE)
                    b_state = torch.cat([x[0] for x in batch])
                    b_action = torch.LongTensor([x[1] for x in batch]).unsqueeze(1)
                    b_reward = torch.FloatTensor([x[2] for x in batch]).unsqueeze(1)
                    b_next = torch.cat([x[3] for x in batch])
                    b_done = torch.FloatTensor([float(x[4]) for x in batch]).unsqueeze(1)

                    curr_q = policy_net(b_state).gather(1, b_action)
                    next_q = target_net(b_next).max(1)[0].unsqueeze(1)
                    expected_q = b_reward + (GAMMA * next_q * (1 - b_done))

                    loss = loss_func(curr_q, expected_q)
                    optimizer.zero_grad()
                    loss.backward()
                    optimizer.step()

            episodes += 1
            epsilon = max(EPSILON_END, epsilon * EPSILON_DECAY)
            
            if episodes % TARGET_UPDATE == 0:
                target_net.load_state_dict(policy_net.state_dict())
                if not render_mode:
                    print(f"\nEpisode {episodes}, Reward: {total_reward:.1f}, Epsilon: {epsilon:.2f}")

    except KeyboardInterrupt:
        print("\nğŸ›‘ å¼·åˆ¶ä¸­æ–·ï¼Œå­˜æª”ä¸­...")
        torch.save(policy_net.state_dict(), "traffic_dqn.pth")
        env.pygame.quit()

if __name__ == "__main__":
    train()